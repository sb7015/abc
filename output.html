<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nanonets-OCR-s: Comprehensive Analysis Report</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #EBE4FF 0%, #D9F5F5 50%, #E1edff 100%);
            min-height: 100vh;
            color: #231E33;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            text-align: center;
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        h1 {
            color: #281849;
            font-size: 3rem;
            font-weight: 300;
            margin-bottom: 15px;
            letter-spacing: 2px;
        }

        .subtitle {
            color: #5009B5;
            font-size: 1.2rem;
            font-weight: 400;
        }

        .nav-tabs {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }

        .tab-button {
            background: #F5F5F5;
            border: none;
            padding: 12px 24px;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            color: #281849;
            font-weight: 500;
            border: 2px solid transparent;
        }

        .tab-button:hover {
            background: #EBE4FF;
            transform: translateY(-2px);
        }

        .tab-button.active {
            background: #5009B5;
            color: white;
            box-shadow: 0 8px 25px rgba(80, 9, 181, 0.3);
        }

        .tab-content {
            display: none;
            animation: fadeIn 0.5s ease-in;
        }

        .tab-content.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .card {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 25px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-3px);
            box-shadow: 0 20px 45px rgba(0, 0, 0, 0.15);
        }

        .card h2 {
            color: #281849;
            font-size: 1.8rem;
            margin-bottom: 20px;
            border-bottom: 3px solid #794CFF;
            padding-bottom: 10px;
            display: inline-block;
        }

        .card h3 {
            color: #5009B5;
            font-size: 1.3rem;
            margin: 25px 0 15px 0;
        }

        .card h4 {
            color: #00BEBA;
            font-size: 1.1rem;
            margin: 20px 0 10px 0;
        }

        .highlight {
            background: linear-gradient(135deg, #EBE4FF, #D9F5F5);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #5009B5;
            margin: 20px 0;
        }

        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .feature-card {
            background: linear-gradient(135deg, #F5F5F5, #EBE4FF);
            padding: 20px;
            border-radius: 12px;
            border: 2px solid transparent;
            transition: all 0.3s ease;
        }

        .feature-card:hover {
            border-color: #794CFF;
            transform: translateY(-3px);
            box-shadow: 0 10px 25px rgba(121, 76, 255, 0.2);
        }

        .feature-card h4 {
            color: #281849;
            margin-bottom: 10px;
        }

        .code-block {
            background: #281849;
            color: #EBE4FF;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            margin: 20px 0;
            border: 2px solid #5009B5;
        }

        .architecture-diagram {
            background: linear-gradient(135deg, #D9F5F5, #E1edff);
            padding: 30px;
            border-radius: 15px;
            text-align: center;
            margin: 25px 0;
            border: 2px solid #00BEBA;
        }

        .workflow-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 8px;
            border-left: 4px solid #00B5F5;
            box-shadow: 0 5px 15px rgba(0, 181, 245, 0.1);
        }

        .performance-metric {
            display: inline-block;
            background: #5009B5;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            margin: 5px;
            font-size: 0.9rem;
            font-weight: 500;
        }

        .limitation {
            background: rgba(255, 255, 255, 0.7);
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #794CFF;
            margin: 10px 0;
        }

        .source-link {
            color: #00BEBA;
            text-decoration: none;
            font-weight: 500;
            border-bottom: 1px dotted #00BEBA;
            transition: all 0.3s ease;
        }

        .source-link:hover {
            color: #5009B5;
            border-bottom-color: #5009B5;
        }

        ul, ol {
            padding-left: 25px;
        }

        li {
            margin: 8px 0;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .stat-card {
            background: linear-gradient(135deg, #00BEBA, #00B5F5);
            color: white;
            padding: 20px;
            border-radius: 12px;
            text-align: center;
            transform: perspective(1000px) rotateY(0deg);
            transition: transform 0.3s ease;
        }

        .stat-card:hover {
            transform: perspective(1000px) rotateY(5deg) translateY(-5px);
        }

        .stat-number {
            font-size: 2rem;
            font-weight: bold;
            display: block;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
        }

        .comparison-table th {
            background: #5009B5;
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #F5F5F5;
        }

        .comparison-table tr:hover {
            background: #EBE4FF;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }

            h1 {
                font-size: 2rem;
            }

            .header {
                padding: 25px;
            }

            .nav-tabs {
                flex-direction: column;
                align-items: center;
            }

            .feature-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Nanonets-OCR-s</h1>
            <p class="subtitle">Comprehensive Analysis & Code Understanding</p>
        </div>

        <div class="nav-tabs">
            <button class="tab-button active" onclick="showTab('overview')">Overview</button>
            <button class="tab-button" onclick="showTab('code-analysis')">Code Analysis</button>
            <button class="tab-button" onclick="showTab('architecture')">Architecture</button>
            <button class="tab-button" onclick="showTab('capabilities')">Capabilities</button>
            <button class="tab-button" onclick="showTab('performance')">Performance</button>
            <button class="tab-button" onclick="showTab('applications')">Applications</button>
        </div>

        <div id="overview" class="tab-content active">
            <div class="card">
                <h2>Model Overview</h2>
                <div class="highlight">
                    <p><strong>Nanonets-OCR-s</strong> is a state-of-the-art image-to-markdown OCR model developed by Nanonets that goes far beyond traditional text extraction. It's a 3.75B parameter Vision-Language Model (VLM) based on Qwen2.5-VL-3B architecture that transforms documents into structured markdown with intelligent content recognition and semantic tagging.</p>
                </div>

                <div class="stats-grid">
                    <div class="stat-card">
                        <span class="stat-number">3.75B</span>
                        <span>Parameters</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-number">250K+</span>
                        <span>Training Pages</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-number">7</span>
                        <span>Key Features</span>
                    </div>
                </div>

                <h3>Key Differentiators</h3>
                <div class="feature-grid">
                    <div class="feature-card">
                        <h4>🧠 Semantic Understanding</h4>
                        <p>Goes beyond plain text extraction to understand document structure and context</p>
                    </div>
                    <div class="feature-card">
                        <h4>📊 Structured Output</h4>
                        <p>Generates properly formatted markdown with semantic tagging</p>
                    </div>
                    <div class="feature-card">
                        <h4>🔄 LLM-Ready</h4>
                        <p>Output optimized for downstream processing by Large Language Models</p>
                    </div>
                </div>
            </div>
        </div>

        <div id="code-analysis" class="tab-content">
            <div class="card">
                <h2>Code Analysis</h2>
                
                <h3>Example 1: Table Extraction Pipeline</h3>
                <p>The first code example focuses on extracting tables from PDF documents and saving them as CSV or Excel files.</p>
                
                <div class="workflow-step">
                    <strong>Step 1:</strong> Load model components (model, tokenizer, processor)
                </div>
                <div class="workflow-step">
                    <strong>Step 2:</strong> Convert PDF pages to JPEG images using pdf2image
                </div>
                <div class="workflow-step">
                    <strong>Step 3:</strong> Process each image through OCR model with table extraction prompt
                </div>
                <div class="workflow-step">
                    <strong>Step 4:</strong> Parse HTML output to extract structured tables using pandas.read_html()
                </div>
                <div class="workflow-step">
                    <strong>Step 5:</strong> Combine tables from all pages and export as CSV/Excel
                </div>

                <div class="code-block">
def process_pdf_for_tables(pdf_path, model, processor, output_dir, output_format="csv"):
    os.makedirs(output_dir, exist_ok=True)
    images = convert_from_path(pdf_path)
    all_tables = []

    for i, image in enumerate(images):
        html_output = ocr_page_with_nanonets_s(image, model, processor)
        tables = pd.read_html(StringIO(html_output))
        for j, table in enumerate(tables):
            table['Page'] = i + 1
            all_tables.append(table)
                </div>

                <h3>Example 2: Comprehensive Document Processing</h3>
                <p>The second code example provides a more comprehensive OCR solution for general document processing.</p>
                
                <div class="highlight">
                    <h4>Enhanced Prompt Engineering:</h4>
                    <ul>
                        <li>Extract text naturally as if reading</li>
                        <li>Return tables in HTML format</li>
                        <li>Convert equations to LaTeX representation</li>
                        <li>Describe images with structured tags</li>
                        <li>Handle watermarks, page numbers, and checkboxes</li>
                    </ul>
                </div>
            </div>
        </div>

        <div id="architecture" class="tab-content">
            <div class="card">
                <h2>Model Architecture</h2>
                
                <div class="architecture-diagram">
                    <h3>Based on Qwen2.5-VL-3B Foundation</h3>
                    <p>Built on the robust Qwen2.5-VL architecture with specialized fine-tuning for OCR tasks</p>
                </div>

                <h3>Vision Transformer (ViT) Components</h3>
                <div class="feature-grid">
                    <div class="feature-card">
                        <h4>Dynamic Resolution Processing</h4>
                        <p>Handles images of varying sizes without traditional normalization, enabling native resolution perception</p>
                    </div>
                    <div class="feature-card">
                        <h4>Window Attention</h4>
                        <p>Strategic implementation reduces computational overhead while maintaining performance</p>
                    </div>
                    <div class="feature-card">
                        <h4>Optimized Architecture</h4>
                        <p>Enhanced with SwiGLU and RMSNorm, aligning with Qwen2.5 LLM structure</p>
                    </div>
                </div>

                <h3>Advanced Features</h3>
                <ul>
                    <li><strong>mRoPE (Multimodal Rotary Position Embedding):</strong> Facilitates effective fusion of positional information across text, images, and videos</li>
                    <li><strong>Flash Attention 2:</strong> Optimizes attention mechanisms for faster training and inference</li>
                    <li><strong>Native Dynamic Resolution:</strong> Processes images without relying on traditional normalization techniques</li>
                </ul>

                <h3>Training Methodology</h3>
                <div class="workflow-step">
                    <strong>Dataset Size:</strong> Over 250,000 pages of diverse document types
                </div>
                <div class="workflow-step">
                    <strong>Document Types:</strong> Research papers, financial documents, legal documents, healthcare documents, tax forms, receipts, and invoices
                </div>
                <div class="workflow-step">
                    <strong>Training Process:</strong> Initial training on synthetic datasets, followed by fine-tuning on manually annotated real-world data
                </div>
            </div>
        </div>

        <div id="capabilities" class="tab-content">
            <div class="card">
                <h2>Model Capabilities</h2>
                
                <div class="feature-grid">
                    <div class="feature-card">
                        <h4>📐 LaTeX Equation Recognition</h4>
                        <p>Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. Distinguishes between inline ($...$) and display ($$...$$) equations.</p>
                    </div>
                    <div class="feature-card">
                        <h4>🖼️ Intelligent Image Description</h4>
                        <p>Describes images within documents using structured &lt;img&gt; tags, making them digestible for LLM processing. Can describe logos, charts, graphs with content, style, and context details.</p>
                    </div>
                    <div class="feature-card">
                        <h4>📋 Table Extraction</h4>
                        <p>Extracts complex tables from documents and converts them into markdown and HTML tables while maintaining structure and relationships.</p>
                    </div>
                    <div class="feature-card">
                        <h4>✍️ Signature Detection</h4>
                        <p>Identifies and isolates signatures from other text, outputting them within &lt;signature&gt; tags for separate processing.</p>
                    </div>
                    <div class="feature-card">
                        <h4>🏷️ Watermark Extraction</h4>
                        <p>Detects and extracts watermark text within appropriate tags for identification and handling.</p>
                    </div>
                    <div class="feature-card">
                        <h4>☑️ Checkbox Recognition</h4>
                        <p>Converts form checkboxes and radio buttons into standardized Unicode symbols. Predicts checkbox status within &lt;checkbox&gt; tags.</p>
                    </div>
                </div>

                <h3>Technical Specifications</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Specification</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Parameters</td>
                            <td>3.75B (based on Qwen2.5-VL-3B)</td>
                        </tr>
                        <tr>
                            <td>Framework</td>
                            <td>HuggingFace Transformers</td>
                        </tr>
                        <tr>
                            <td>Input Formats</td>
                            <td>Images (JPEG, PNG), PDF documents</td>
                        </tr>
                        <tr>
                            <td>Output Format</td>
                            <td>Structured Markdown with semantic tags</td>
                        </tr>
                        <tr>
                            <td>Hardware Requirements</td>
                            <td>Modern GPUs (A100, RTX 4090) recommended</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div id="performance" class="tab-content">
            <div class="card">
                <h2>Performance & Benchmarks</h2>
                
                <h3>Industry Recognition</h3>
                <div class="highlight">
                    <p>Recognized as <strong>"the best OCR AI model ever"</strong> by industry experts, demonstrating superior structured output quality compared to leading solutions.</p>
                </div>

                <h3>Benchmark Performance</h3>
                <div class="performance-metric">Superior Structured Output</div>
                <div class="performance-metric">Consistent Markdown Generation</div>
                <div class="performance-metric">No Complex Prompt Engineering</div>
                <div class="performance-metric">Document Structure Preservation</div>

                <h3>Comparative Analysis</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Structured Output</th>
                            <th>Semantic Tagging</th>
                            <th>Prompt Complexity</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Nanonets-OCR-s</strong></td>
                            <td>✅ Superior</td>
                            <td>✅ Advanced</td>
                            <td>✅ Minimal</td>
                        </tr>
                        <tr>
                            <td>Donut</td>
                            <td>⚠️ Basic</td>
                            <td>⚠️ Limited</td>
                            <td>❌ Complex</td>
                        </tr>
                        <tr>
                            <td>Dolphin</td>
                            <td>⚠️ Basic</td>
                            <td>❌ None</td>
                            <td>⚠️ Moderate</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Real-world Performance</h3>
                <ul>
                    <li><strong>Generalization:</strong> Shows robust capability on handwritten notes despite not being specifically trained on handwritten content</li>
                    <li><strong>Efficiency:</strong> Remarkable performance as a 3B parameter model for complex OCR tasks</li>
                    <li><strong>Quality:</strong> Maintains document structure and semantic meaning across diverse document types</li>
                </ul>

                <h3>Current Limitations</h3>
                <div class="limitation">
                    <strong>Handwritten Text:</strong> Not specifically trained on handwritten content, though shows some capability
                </div>
                <div class="limitation">
                    <strong>Hallucination:</strong> May occasionally generate content when visual context is ambiguous
                </div>
                <div class="limitation">
                    <strong>Hardware Requirements:</strong> Requires modern GPUs for optimal real-time performance
                </div>
            </div>
        </div>

        <div id="applications" class="tab-content">
            <div class="card">
                <h2>Industry Applications</h2>
                
                <div class="feature-grid">
                    <div class="feature-card">
                        <h4>🎓 Academic & Research</h4>
                        <p>Digitizes papers with LaTeX equations and tables. Preserves mathematical notation and research document structure for academic workflows.</p>
                    </div>
                    <div class="feature-card">
                        <h4>⚖️ Legal & Financial</h4>
                        <p>Extracts data from contracts and financial documents, including signatures and tables. Maintains document integrity and legal compliance.</p>
                    </div>
                    <div class="feature-card">
                        <h4>🏥 Healthcare & Pharma</h4>
                        <p>Accurately captures text and checkboxes from medical forms. Handles complex medical documentation requirements with precision.</p>
                    </div>
                    <div class="feature-card">
                        <h4>🏢 Corporate & Enterprise</h4>
                        <p>Transforms reports into searchable, image-aware knowledge bases. Enables enterprise-wide document digitization and automation.</p>
                    </div>
                </div>

                <h3>Integration Options</h3>
                <div class="workflow-step">
                    <strong>HuggingFace Hub:</strong> Direct model access and inference capabilities
                </div>
                <div class="workflow-step">
                    <strong>Docext Tool:</strong> Integrated with docext for immediate use and testing
                </div>
                <div class="workflow-step">
                    <strong>API Integration:</strong> Compatible with OpenAI-style API interfaces
                </div>
                <div class="workflow-step">
                    <strong>Local Deployment:</strong> Self-hosted options available for enterprise use
                </div>

                <h3>Use Case Examples</h3>
                <ul>
                    <li><strong>Document Digitization:</strong> Converting legacy paper documents to searchable digital formats</li>
                    <li><strong>Research Paper Processing:</strong> Extracting equations, tables, and figures from academic publications</li>
                    <li><strong>Form Processing:</strong> Automated extraction of data from surveys, applications, and questionnaires</li>
                    <li><strong>Financial Document Analysis:</strong> Processing invoices, receipts, and financial statements</li>
                    <li><strong>Knowledge Base Creation:</strong> Building searchable repositories from document collections</li>
                </ul>

                <h3>Key Sources & References</h3>
                <p>
                    <a href="https://huggingface.co/nanonets/Nanonets-OCR-s" class="source-link">HuggingFace Model Page</a> |
                    <a href="https://nanonets.com/research/nanonets-ocr-s/" class="source-link">Official Research Documentation</a> |
                    <a href="https://github.com/NanoNets/docext" class="source-link">Docext GitHub Repository</a> |
                    <a href="https://benchmarking.nanonets.com/" class="source-link">IDP Leaderboard</a> |
                    <a href="https://learnopencv.com/nanonets-ocr-s/" class="source-link">Technical Analysis by LearnOpenCV</a>
                </p>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            const contents = document.querySelectorAll('.tab-content');
            contents.forEach(content => content.classList.remove('active'));
            
            // Remove active class from all buttons
            const buttons = document.querySelectorAll('.tab-button');
            buttons.forEach(button => button.classList.remove('active'));
            
            // Show selected tab content
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');
        }

        // Add smooth scrolling and animations
        document.addEventListener('DOMContentLoaded', function() {
            // Animate cards on scroll
            const cards = document.querySelectorAll('.card');
            const observerOptions = {
                threshold: 0.1,
                rootMargin: '0px 0px -50px 0px'
            };

            const observer = new IntersectionObserver(function(entries) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.opacity = '1';
                        entry.target.style.transform = 'translateY(0)';
                    }
                });
            }, observerOptions);

            cards.forEach(card => {
                card.style.opacity = '0';
                card.style.transform = 'translateY(30px)';
                card.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
                observer.observe(card);
            });
        });
    </script>
</body>
</html>