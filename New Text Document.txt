# ----------------------------------------------------------
# 0.  One-time installs (restart kernel afterwards if prompted)
# ----------------------------------------------------------
import subprocess, sys, os
os.environ["PIP_CACHE_DIR"] = "/tmp/pip_cache"
subprocess.check_call([
    sys.executable, "-m", "pip", "install", "-q", "--upgrade",
    "mlx-vlm", "docling-core", "pymupdf", "pillow",
    "langchain", "langchain-community", "faiss-cpu", "sentence-transformers"
])

# ----------------------------------------------------------
# 1.  Imports
# ----------------------------------------------------------
import io, pathlib, re, warnings, torch
import fitz  # PyMuPDF
from PIL import Image
from mlx_vlm import load, generate
from mlx_vlm.prompt_utils import apply_chat_template
from mlx_vlm.utils import load_config, stream_generate
from docling_core.types.doc.document import DoclingDocument
from docling_core.types.doc import DocTagsDocument

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from transformers import pipeline

warnings.filterwarnings("ignore")

# ----------------------------------------------------------
# 2.  Load SmolDocling model (tiny, local, CPU-friendly)
# ----------------------------------------------------------
MODEL_ID = "ds4sd/SmolDocling-256M-preview-mlx-bf16"
print("Loading SmolDocling â€¦")
model, processor = load(MODEL_ID)
config = load_config(MODEL_ID)

# ----------------------------------------------------------
# 3.  PDF â†’ Markdown  (page-by-page via SmolDocling)
# ----------------------------------------------------------
PDF_PATH = pathlib.Path("/mnt/nfs/work/Nanonets/Notebook/mwqa.pdf")

def pdf_to_images(pdf_path, dpi=144):
    doc = fitz.open(str(pdf_path))
    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        yield Image.open(io.BytesIO(pix.tobytes("png")))

markdown_pages = []
for idx, img in enumerate(pdf_to_images(PDF_PATH), 1):
    prompt = "Convert this page to docling."
    formatted = apply_chat_template(processor, config, prompt, num_images=1)

    # generate DocTags
    output = ""
    for token in stream_generate(model, processor, formatted, [img],
                                 max_tokens=1024, verbose=False):
        output += token.text

    # convert to final markdown
    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([output], [img])
    docling_doc = DoclingDocument(name=f"Page_{idx}")
    docling_doc.load_from_doctags(doctags_doc)
    markdown_pages.append(docling_doc.export_to_markdown())
    print(f"âœ… page {idx} done")

# ----------------------------------------------------------
# 4.  Clean text (strip artefacts, collapse whitespace)
# ----------------------------------------------------------
raw_text = "\n\n".join(markdown_pages)
text = re.sub(r'<[^>]+>', '', raw_text)
text = re.sub(r'[a-zA-Z0-9\-]+\s*\{[^}]*\}', '', text)
text = " ".join(text.split())
print(f"âœ… Clean text length: {len(text):,} chars")

# ----------------------------------------------------------
# 5.  Build QA chain (FAISS + T5)
# ----------------------------------------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def build_qa_chain(txt: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.create_documents([txt])

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": DEVICE}
    )
    vectorstore = FAISS.from_documents(docs, embeddings)

    llm_pipe = pipeline(
        "text2text-generation",
        model="google/flan-t5-large",
        device=0 if DEVICE=="cuda" else -1,
        max_new_tokens=256,
    )
    return RetrievalQA.from_chain_type(
        llm=llm_pipe,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=False,
    )

qa_chain = build_qa_chain(text)

# ----------------------------------------------------------
# 6.  Ask any question
# ----------------------------------------------------------
question = "What is the main topic of the document?"
answer = qa_chain.invoke({"query": question})
print("ðŸ§  Q:", question)
print("ðŸ“„ A:", answer["result"])
