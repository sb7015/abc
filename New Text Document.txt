# main.ipynb  (or main.py)
# ------------------------------------------------------------------
# PARAMETERS – change these two strings only
pdf_name   = "Winnebago_County_1.pdf"   # PDF sitting in /SAN/
# ------------------------------------------------------------------

import os, json, shutil, gc, torch
from pathlib import Path

# ----------  ENV + GPU  ----------
os.environ['HF_MODEL_CACHE'] = "/mnt/nfs/work/Abhiram/qwen7b"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:128"
torch.cuda.empty_cache()
gc.collect()

# ----------  PATHS  ----------
SAN_ROOT = Path("/mnt/nfs/work/Abhiram/SAN")
pdf_path = SAN_ROOT / pdf_name
stem     = pdf_path.stem
work_dir = SAN_ROOT / stem
index_dir = work_dir / "index"

work_dir.mkdir(parents=True, exist_ok=True)
index_dir.mkdir(parents=True, exist_ok=True)

# ----------  INDEX  ----------
from byaldi import RAGMultiModalModel

print(f"Indexing {pdf_path.name} → {index_dir}")
rag = RAGMultiModalModel.from_pretrained("vidore/colpali", verbose=1)  # Enable verbose logging

# Ensure the input path is a single string
input_path = str(pdf_path)
print(f"Input path: {input_path}")

# Ensure doc_ids list matches the number of input items
doc_ids = ["0"]  # Assuming a single PDF
print(f"Doc IDs: {doc_ids}")

# Convert doc_ids to integers if necessary
doc_ids = [int(doc_id) for doc_id in doc_ids]

try:
    rag.index(
        input_path=input_path,
        index_name=stem,
        doc_ids=doc_ids,
        store_collection_with_index=True,
        overwrite=True,
    )
except Exception as e:
    print(f"Error during indexing: {e}")
    raise

# Move generated index files into <stem>/index/
src_index = Path.cwd() / stem
if src_index.exists():
    for item in src_index.iterdir():
        shutil.move(str(item), index_dir / item.name)
    src_index.rmdir()

# ----------  pdf_mapping.json  ----------
pdf_mapping = {"pdf_mapping": {"0": str(pdf_path.resolve())}}
(work_dir / "pdf_mapping.json").write_text(json.dumps(pdf_mapping, indent=2))

print("✅ Done – index & mapping saved under", work_dir)





# ------------------------------------------------------------------
# PARAMETERS – change these four strings/ints only
pdf_name       = "Winnebago_County_1"   # folder name (same as PDF stem)
target_column  = "Key"                  # column to query
model_choice   = "gpt"                 # "gpt" | "qwen" | "both" | "none"
top_k          = 4                      # pages per query
# ------------------------------------------------------------------

import os, json, time, gc, torch
from pathlib import Path
from typing import List, Tuple
import pandas as pd
from PIL import Image
from byaldi import RAGMultiModalModel
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from openai import OpenAI
import base64
import io
from pdf2image import convert_from_path

# ---------- ENV & GPU ----------
os.environ['HF_MODEL_CACHE'] = "/mnt/nfs/work/Abhiram/qwen7b"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
torch.cuda.empty_cache()
gc.collect()

# ---------- PATHS ----------
SAN_ROOT   = Path("/mnt/nfs/work/Abhiram/SAN")
work_dir   = SAN_ROOT / pdf_name
default_index_dir = Path.cwd() / ".byaldi" / pdf_name
excel_path = work_dir / "VALIDATION_Winnebago.xlsx"
result_dir = work_dir / f"{pdf_name}_result"
result_dir.mkdir(parents=True, exist_ok=True)

if not excel_path.exists():
    raise FileNotFoundError(f"Excel file not found: {excel_path}")

# ---------- LOAD RAG INDEX ----------
try:
    rag = RAGMultiModalModel.from_index(str(default_index_dir))
    print(f"Loaded ColPali index from {default_index_dir}")
except Exception as e:
    raise RuntimeError(f"Error loading ColPali index: {e}")

# ---------- LOAD QWEN MODEL ----------
qwen_model, qwen_processor = None, None
if model_choice in ["qwen", "both"]:
    try:
        qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            torch_dtype="auto",
            device_map="auto",
            cache_dir=os.environ['HF_MODEL_CACHE'],
        )
        qwen_processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
        print("Loaded Qwen-2.5-VL model")
    except Exception as e:
        raise RuntimeError(f"Error loading Qwen model: {e}")

# ---------- LOAD OPENAI ----------
openai_client = None
if model_choice in ["gpt", "both"]:
    try:
        openai_client = OpenAI(
            api_key="LLMAICoEUS#24w@GLD",
            base_url="https://llm-perf.elevancehealth.com/api/openai/"
        )
        print("Loaded OpenAI client")
    except Exception as e:
        raise RuntimeError(f"Error loading OpenAI client: {e}")

# ---------- READ EXCEL ----------
try:
    df = pd.read_excel(excel_path)
    if target_column not in df.columns:
        raise ValueError(f"Column '{target_column}' not found in Excel")
    print(f"Loaded Excel file with {len(df)} rows")
except Exception as e:
    raise RuntimeError(f"Error reading Excel: {e}")

# ---------- LOAD PDF IMAGES ----------
try:
    with open(work_dir / "pdf_mapping.json") as f:
        pdf_mapping = json.load(f)["pdf_mapping"]
    imgs = {doc_id: convert_from_path(pdf_path) for doc_id, pdf_path in pdf_mapping.items()}
    print("Loaded images from PDFs")
except Exception as e:
    raise RuntimeError(f"Error loading images: {e}")

# ---------- IMAGE COMPRESSION ----------
def compress_image(img: Image.Image, max_h: int = 700) -> Image.Image:
    w = int(max_h * img.width / img.height)
    return img.resize((w, max_h), Image.LANCZOS)

# ---------- GPT-4o CALL ----------
def call_gpt(query: str, images: List[Image.Image]) -> Tuple[str, int, float]:
    start = time.time()
    payloads = [
        {"type": "text", "text": base64.b64encode(io.BytesIO(img.save(io.BytesIO(), format="PNG")).getvalue()).decode()}
        for img in images
    ]
    messages = [{"role": "user", "content": [{"type": "text", "text": query}] + payloads}]
    try:
        resp = openai_client.chat.completions.create(model="gpt-4o-openai", messages=messages)
        print(resp)
        return resp.choices[0].message.content, resp.usage.total_tokens, time.time() - start
    except Exception as e:
        raise RuntimeError(f"Error calling GPT-4o: {e}")

# ---------- QWEN CALL ----------
def call_qwen(query: str, images: List[Image.Image]) -> Tuple[str, int, float]:
    start = time.time()
    vision_inputs, _ = process_vision_info([{"type": "image", "image": compress_image(img)} for img in images])
    text = qwen_processor.apply_chat_template([{"role": "user", "content": [{"type": "text", "text": query}] + vision_inputs}], tokenize=False)
    inputs = qwen_processor(text=[text], images=vision_inputs, return_tensors="pt").to("cuda")
    try:
        generated = qwen_model.generate(**inputs, max_new_tokens=512)
        decoded = qwen_processor.batch_decode(generated, skip_special_tokens=True)[0]
        return decoded, len(generated[0]), time.time() - start
    except Exception as e:
        raise RuntimeError(f"Error calling Qwen: {e}")

# ---------- MAIN EXECUTION ----------
def run(model_choice: str):
    img_indices, doc_ids = [[] for _ in range(top_k)], [[] for _ in range(top_k)]

    for query in df[target_column]:
        results = rag.search(query, k=top_k)
        for k in range(top_k):
            img_indices[k].append(results[k]["page_num"] - 1)
            doc_ids[k].append(results[k]["doc_id"])

    for k in range(top_k):
        df[f"image_index{k}"] = img_indices[k]
        df[f"doc_id{k}"] = doc_ids[k]

    gpt_rows, qwen_rows = [], []
    for _, row in df.iterrows():
        query = row[target_column]
        images = [compress_image(imgs[str(row[f"doc_id{k}"])][row[f"image_index{k}"]]) for k in range(top_k)]

        if model_choice in ["gpt", "both"]:
            ans, toks, t = call_gpt(query, images)
            gpt_rows.append({**row.to_dict(), "response": ans, "gpt_tokens": toks, "gpt_time": round(t, 2)})

        if model_choice in ["qwen", "both"]:
            ans, toks, t = call_qwen(query, images)
            qwen_rows.append({**row.to_dict(), "response": ans, "qwen_tokens": toks, "qwen_time": round(t, 2)})

    # ---------- SAVE RESULTS ----------
    if model_choice == "none":
        df.to_csv(result_dir / f"{pdf_name}_validation.csv", index=False)
        df.to_json(result_dir / f"{pdf_name}_validation.json", orient="records", indent=2)
        print("Saved validation results")

    if model_choice == "gpt":
        output_path = result_dir / f"LLM_GPT_RESPONSE_{pdf_name}.xlsx"
        pd.DataFrame(gpt_rows).to_excel(output_path, index=False)
        print(f"Saved GPT responses to {output_path}")

    if model_choice == "qwen":
        output_path = result_dir / f"LLM_QWEN_RESPONSE_{pdf_name}.xlsx"
        pd.DataFrame(qwen_rows).to_excel(output_path, index=False)
        print(f"Saved Qwen responses to {output_path}")

    if model_choice == "both":
        gpt_path = result_dir / f"LLM_GPT_RESPONSE_{pdf_name}.xlsx"
        qwen_path = result_dir / f"LLM_QWEN_RESPONSE_{pdf_name}.xlsx"
        pd.DataFrame(gpt_rows).to_excel(gpt_path, index=False)
        pd.DataFrame(qwen_rows).to_excel(qwen_path, index=False)
        print(f"Saved both GPT and Qwen responses to:\n - {gpt_path}\n - {qwen_path}")

# ---------- RUN ----------
run(model_choice)
