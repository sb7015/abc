#!/usr/bin/env python3
"""
OpenAI-provider LangExtract benchmark (no os.getenv, no API key check)
"""

import time, json, csv, textwrap, logging
from datetime import datetime
from typing import List, Dict, Any

import langextract as lx

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("openai_benchmark")

# ------------------------------------------------------------------
# 1. CONFIG â€“ EDIT HERE ONLY
# ------------------------------------------------------------------
API_KEY      = "XXXXXXXXX"   # Replace with your actual key
MODEL_URL    = "XXXXXXXXX"
MODEL_ID     = "gpt-4o"
TIMEOUT_S    = 60
MAX_CHARS    = 200_000

# ------------------------------------------------------------------
# 2. Prompt + few-shot
# ------------------------------------------------------------------
PROMPT = textwrap.dedent("""\
Extract planet names, numeric facts, and relationships.
- extraction_class: planet_fact
- extraction_text: exact phrase
- attributes: {fact: short phrase}\
""")

EXAMPLES = [
    lx.data.ExampleData(
        text="Mars has a radius of 3,390 km.",
        extractions=[
            lx.data.Extraction(
                extraction_class="planet_fact",
                extraction_text="Mars",
                attributes={"fact": "radius 3,390 km"},
            )
        ],
    )
]

# ------------------------------------------------------------------
# 3. Corpus builder
# ------------------------------------------------------------------
def build_corpus() -> List[Dict[str, str]]:
    base = """
    Mercury is the smallest planet. Venus is the hottest. Earth harbors life.
    Mars is red. Jupiter is massive. Saturn has rings. Uranus rolls on its side.
    Neptune has strong winds. Pluto is a dwarf planet.
    """
    docs = []
    for mult in (1, 3, 10, 30, 100, 300):
        txt = (base * mult).strip()
        if len(txt) > MAX_CHARS:
            break
        docs.append({"name": f"{len(txt)//1_000}k", "text": txt})
    return docs

# ------------------------------------------------------------------
# 4. Single call
# ------------------------------------------------------------------
def single_run(text: str) -> Dict[str, Any]:
    start = time.perf_counter()
    try:
        result = lx.extract(
            text_or_documents=text,
            prompt_description=PROMPT,
            examples=EXAMPLES,
            model_id=MODEL_ID,
            api_key=API_KEY,
            model_url=MODEL_URL,
            fence_output=True,
            use_schema_constraints=False,
        )
        elapsed = time.perf_counter() - start
        extractions = result.extractions or []
        usage = getattr(result, "usage_metadata", {}) or {}
        prompt_tokens = usage.get("prompt_token_count", 0)
        completion_tokens = usage.get("candidates_token_count", 0)
        return {
            "success": True,
            "chars": len(text),
            "elapsed_s": round(elapsed, 2),
            "extractions": len(extractions),
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "error": None,
        }
    except Exception as exc:
        elapsed = time.perf_counter() - start
        return {
            "success": False,
            "chars": len(text),
            "elapsed_s": round(elapsed, 2),
            "extractions": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "error": str(exc)[:100],
        }

# ------------------------------------------------------------------
# 5. Main loop
# ------------------------------------------------------------------
def main():
    corpus = build_corpus()
    log.info("Starting benchmark with %s documents", len(corpus))

    csv_name = f"openai_langextract_benchmark_{datetime.now():%Y%m%d_%H%M}.csv"
    jsonl_name = csv_name.replace(".csv", ".jsonl")

    with open(csv_name, "w", newline="", encoding="utf-8") as cf, open(
        jsonl_name, "w", encoding="utf-8"
    ) as jf:
        writer = csv.DictWriter(
            cf,
            fieldnames=[
                "name",
                "chars",
                "elapsed_s",
                "extractions",
                "prompt_tokens",
                "completion_tokens",
                "success",
                "error",
            ],
        )
        writer.writeheader()

        for doc in corpus:
            log.info("Running %s (%s chars)", doc["name"], f"{len(doc['text']):,}")
            res = single_run(doc["text"])
            row = {"name": doc["name"], **res}
            writer.writerow(row)
            cf.flush()

            json_rec = {
                "timestamp": datetime.now().isoformat(),
                "model_id": MODEL_ID,
                "model_url": MODEL_URL,
                **row,
            }
            print(json.dumps(json_rec), file=jf)
            jf.flush()

            if not res["success"] or res["elapsed_s"] > TIMEOUT_S:
                log.warning("Stopping early â€“ last call failed or timed out")
                break

    log.info("Done. Results -> %s + %s", csv_name, jsonl_name)


if __name__ == "__main__":
    main()
name	chars	elapsed_s	extractions	prompt_tokens	completion_tokens	success	error
0k	209	1.98	9	0	0	TRUE	
0k	647	1.85	9	0	0	TRUE	
2k	2180	3.37	27	0	0	TRUE	
6k	6560	9.04	99	0	0	TRUE	
21k	21890	723.17	207	0	0	TRUE	



import os
import textwrap
import pathlib as pl
import pandas as pd
import langextract as lx

# Set up SSL certificates if needed
os.environ['SSL_CERT_FILE'] = os.path.join(os.getcwd(), "root.pem")
os.environ['REQUESTS_CA_BUNDLE'] = os.path.join(os.getcwd(), "root.pem")

# ðŸ”§ Set your markdown filename here
markdown_filename = "extracted_pages_combined.md"  # Replace with your actual file name
md_file = pl.Path(markdown_filename)

if not md_file.exists():
    raise FileNotFoundError(f"File not found: {md_file.resolve()}")

# ðŸ“Œ Prompt and few-shot example
prompt = textwrap.dedent("""\
    The input is a Markdown file that contains one or more tables (pipe-delimited or GitHub-flavoured).
    1. Locate the primary data table (ignore layout or TOC tables).
    2. Use the header row to determine the column names.
    3. Return one extraction per data row (skip the header itself).
    4. extraction_class must be "table_row".
    5. extraction_text must be the raw text of the first cell in that row (useful as a natural key).
    6. attributes must be a dict whose keys are the exact column headers and whose values are the corresponding cell texts for that row.
    7. Preserve original wording; do not paraphrase.
    8. If a cell is empty, store an empty string as value.
    9. Emit rows in the same order they appear in the Markdown.
""")

examples = [
    lx.data.ExampleData(
        text=textwrap.dedent("""\
            | Planet  | Distance (AU) | Radius (km) | Moons |
            |---------|---------------|-------------|-------|
            | Earth   | 1.0           | 6371        | 1     |
            | Mars    | 1.52          | 3390        | 2     |
        """),
        extractions=[
            lx.data.Extraction(
                extraction_class="table_row",
                extraction_text="Earth",
                attributes={
                    "Planet": "Earth",
                    "Distance (AU)": "1.0",
                    "Radius (km)": "6371",
                    "Moons": "1"
                }
            ),
            lx.data.Extraction(
                extraction_class="table_row",
                extraction_text="Mars",
                attributes={
                    "Planet": "Mars",
                    "Distance (AU)": "1.52",
                    "Radius (km)": "3390",
                    "Moons": "2"
                }
            ),
        ]
    )
]

# ðŸ“„ Read the Markdown file
markdown_text = md_file.read_text(encoding="utf-8")

# ðŸ§  LangExtract call
result = lx.extract(
    text_or_documents=markdown_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",
    api_key="hclXrQFZfwREJigLbW6wKu3ao68Bwyk6",  # or replace with your actual key
    model_url="https://api.horizon.elevancehealth.com/openai/v1"  # or your custom model URL
)

# ðŸ“Š Convert extractions to DataFrame
rows = []
for ext in result.extractions:
    row = {"Class": ext.extraction_class, "Text": ext.extraction_text}
    if ext.attributes:  # Only update if attributes are not None
        row.update(ext.attributes)
    rows.append(row)
df = pd.DataFrame(rows)

# ðŸ§¼ Clean and export
df_clean = df.drop(columns=["Class", "Text"])
df_clean.to_excel(md_file.with_suffix(".xlsx"), index=False)
df_clean.to_html(md_file.with_suffix(".html"), index=False)

print(f"âœ… Done! Files saved as:\nâ†’ {md_file.with_suffix('.xlsx').resolve()}\nâ†’ {md_file.with_suffix('.html').resolve()}")




