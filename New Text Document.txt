#!/usr/bin/env python3
"""
OpenAI-provider LangExtract benchmark (no os.getenv, no API key check)
"""

import time, json, csv, textwrap, logging
from datetime import datetime
from typing import List, Dict, Any

import langextract as lx

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("openai_benchmark")

# ------------------------------------------------------------------
# 1. CONFIG – EDIT HERE ONLY
# ------------------------------------------------------------------
API_KEY      = "XXXXXXXXX"   # Replace with your actual key
MODEL_URL    = "XXXXXXXXX"
MODEL_ID     = "gpt-4o"
TIMEOUT_S    = 60
MAX_CHARS    = 200_000

# ------------------------------------------------------------------
# 2. Prompt + few-shot
# ------------------------------------------------------------------
PROMPT = textwrap.dedent("""\
Extract planet names, numeric facts, and relationships.
- extraction_class: planet_fact
- extraction_text: exact phrase
- attributes: {fact: short phrase}\
""")

EXAMPLES = [
    lx.data.ExampleData(
        text="Mars has a radius of 3,390 km.",
        extractions=[
            lx.data.Extraction(
                extraction_class="planet_fact",
                extraction_text="Mars",
                attributes={"fact": "radius 3,390 km"},
            )
        ],
    )
]

# ------------------------------------------------------------------
# 3. Corpus builder
# ------------------------------------------------------------------
def build_corpus() -> List[Dict[str, str]]:
    base = """
    Mercury is the smallest planet. Venus is the hottest. Earth harbors life.
    Mars is red. Jupiter is massive. Saturn has rings. Uranus rolls on its side.
    Neptune has strong winds. Pluto is a dwarf planet.
    """
    docs = []
    for mult in (1, 3, 10, 30, 100, 300):
        txt = (base * mult).strip()
        if len(txt) > MAX_CHARS:
            break
        docs.append({"name": f"{len(txt)//1_000}k", "text": txt})
    return docs

# ------------------------------------------------------------------
# 4. Single call
# ------------------------------------------------------------------
def single_run(text: str) -> Dict[str, Any]:
    start = time.perf_counter()
    try:
        result = lx.extract(
            text_or_documents=text,
            prompt_description=PROMPT,
            examples=EXAMPLES,
            model_id=MODEL_ID,
            api_key=API_KEY,
            model_url=MODEL_URL,
            fence_output=True,
            use_schema_constraints=False,
        )
        elapsed = time.perf_counter() - start
        extractions = result.extractions or []
        usage = getattr(result, "usage_metadata", {}) or {}
        prompt_tokens = usage.get("prompt_token_count", 0)
        completion_tokens = usage.get("candidates_token_count", 0)
        return {
            "success": True,
            "chars": len(text),
            "elapsed_s": round(elapsed, 2),
            "extractions": len(extractions),
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "error": None,
        }
    except Exception as exc:
        elapsed = time.perf_counter() - start
        return {
            "success": False,
            "chars": len(text),
            "elapsed_s": round(elapsed, 2),
            "extractions": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "error": str(exc)[:100],
        }

# ------------------------------------------------------------------
# 5. Main loop
# ------------------------------------------------------------------
def main():
    corpus = build_corpus()
    log.info("Starting benchmark with %s documents", len(corpus))

    csv_name = f"openai_langextract_benchmark_{datetime.now():%Y%m%d_%H%M}.csv"
    jsonl_name = csv_name.replace(".csv", ".jsonl")

    with open(csv_name, "w", newline="", encoding="utf-8") as cf, open(
        jsonl_name, "w", encoding="utf-8"
    ) as jf:
        writer = csv.DictWriter(
            cf,
            fieldnames=[
                "name",
                "chars",
                "elapsed_s",
                "extractions",
                "prompt_tokens",
                "completion_tokens",
                "success",
                "error",
            ],
        )
        writer.writeheader()

        for doc in corpus:
            log.info("Running %s (%s chars)", doc["name"], f"{len(doc['text']):,}")
            res = single_run(doc["text"])
            row = {"name": doc["name"], **res}
            writer.writerow(row)
            cf.flush()

            json_rec = {
                "timestamp": datetime.now().isoformat(),
                "model_id": MODEL_ID,
                "model_url": MODEL_URL,
                **row,
            }
            print(json.dumps(json_rec), file=jf)
            jf.flush()

            if not res["success"] or res["elapsed_s"] > TIMEOUT_S:
                log.warning("Stopping early – last call failed or timed out")
                break

    log.info("Done. Results -> %s + %s", csv_name, jsonl_name)


if __name__ == "__main__":
    main()
name	chars	elapsed_s	extractions	prompt_tokens	completion_tokens	success	error
0k	209	1.98	9	0	0	TRUE	
0k	647	1.85	9	0	0	TRUE	
2k	2180	3.37	27	0	0	TRUE	
6k	6560	9.04	99	0	0	TRUE	
21k	21890	723.17	207	0	0	TRUE	


